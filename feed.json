{
    "version": "https://jsonfeed.org/version/1",
    "title": "liuliAI",
    "description": "",
    "home_page_url": "https://liuliai.github.io",
    "items": [
        {
            "id": "https://liuliai.github.io/2024/07/20/Learning%20to%20(Learn%20at%20Test%20Time)%20RNNs%20with%20Expressive%20Hidden%20States/",
            "url": "https://liuliai.github.io/2024/07/20/Learning%20to%20(Learn%20at%20Test%20Time)%20RNNs%20with%20Expressive%20Hidden%20States/",
            "title": "继RWKV、Mamba、KAN之后号称超越Transformer的线性架构TTT又来了",
            "date_published": "2024-07-20T03:59:00.000Z",
            "content_html": "<h3>继RWKV、Mamba、KAN之后号称超越Transformer的线性架构TTT又来了</h3>\n<p>最早从 23 年 5 月的<a href=\"https://arxiv.org/abs/2305.13048\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">RWKV</a>(RKWV 系列从 V1 更新到 V6，并且作者确实认真做了不少事情的)，再到去年 12 月的<a href=\"https://arxiv.org/abs/2312.00752\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">Mamba</a>，到今年 4 月的<a href=\"https://arxiv.org/abs/2404.19756\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">KAN</a>，再到 5 月的<a href=\"https://arxiv.org/abs/2405.21060\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">Mamba2</a>，到现在的<a href=\"https://arxiv.org/abs/2407.04620\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">TTT</a>。<br>\n<strong>What KAN I say？Mamba out！T_T </strong></p>\n<p>先简单回顾一下</p>\n<h4>RWKV: Reinventing RNNs for the Transformer Era</h4>\nRWKV（Receptance Weighted Key Value） 模型的架构，和 Transformer 非常类似，也是由多个 RWKV block 组成，最后加一个 language modeling head 输出下一个 token 的分布概率。每个 RWKV block 内部，有一个 Channel Mix 和一个Time Mix 模块。\n<img src=\"/image/RWKV1.jpg\" alt=\"语言建模的RWKV架构\">\nRWKV block 内部的 Time Mixing 和 Channel Mixing 模块：\n<img src=\"/image/RWKV2.jpg\" alt=\"RWKV块内的元素（左）和完整的RWKV残差块，配备有用于语言建模的最终头（右）\">\n首先看Time-Mixing Block。Time-Mixing的目的是“Global Interaction”，对应于Transformer中的Self-Attention。\n<ul>\n<li>R 表示过去的信息，用 Sigmoid 激活，遗忘机制。</li>\n<li>W 和相对位置有关， U 对当前位置信号的补偿。</li>\n<li>WKV 类似 Attention 功能，对位置 t ，表达了过去可学习的加权和。</li>\n</ul>\n<p>其中使用到的 R、K、V 对应于 Transformer 中的 Q、K、V。也就是说，K、V 的含义可以强行看作一致，把 R 当做 Q 来处理就行。只是 RKV 的计算方法有点变化：<br>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mtr><mtd><msub><mi>r</mi><mrow><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>r</mi></mrow></msub><mo>⋅</mo><mo stretchy=\"false\">(</mo><msub><mi>μ</mi><mrow><mi>r</mi></mrow></msub><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi></mrow></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msub><mi>μ</mi><mrow><mi>r</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo>,</mo><mstyle scriptlevel=\"0\"><mspace width=\"1em\"></mspace></mstyle><mo stretchy=\"false\">(</mo><mn>11</mn><mo stretchy=\"false\">)</mo></mtd></mtr><mtr><mtd><msub><mi>k</mi><mrow><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>k</mi></mrow></msub><mo>⋅</mo><mo stretchy=\"false\">(</mo><msub><mi>μ</mi><mrow><mi>k</mi></mrow></msub><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi></mrow></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msub><mi>μ</mi><mrow><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo>,</mo><mstyle scriptlevel=\"0\"><mspace width=\"1em\"></mspace></mstyle><mo stretchy=\"false\">(</mo><mn>12</mn><mo stretchy=\"false\">)</mo></mtd></mtr><mtr><mtd><msub><mi>v</mi><mrow><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>v</mi></mrow></msub><mo>⋅</mo><mo stretchy=\"false\">(</mo><msub><mi>μ</mi><mrow><mi>v</mi></mrow></msub><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi></mrow></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msub><mi>μ</mi><mrow><mi>v</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo>,</mo><mstyle scriptlevel=\"0\"><mspace width=\"1em\"></mspace></mstyle><mo stretchy=\"false\">(</mo><mn>13</mn><mo stretchy=\"false\">)</mo></mtd></mtr></mtable></math></p>\n<p>首先，输入经过 LayerNorm 后，将当前位置和前一个位置的输入按权重做一个 Mix，然后分别投影成 R, K, V (公式 11,12,13)。可以看到，这里就是在投影前把历史信息 Mix 起来了，越久的历史，其权重就越小（衰减速率由 μ 控制）。R 通过非线性函数 Sigmoid，得到的结果叫 Receptance，我的理解有点类似于 RNN 里的 Forget Gate 。</p>\n<p>然后是最重要的 Attention 用了如下方法计算：<br>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>w</mi><mi>k</mi><msub><mi>v</mi><mrow><mi>t</mi></mrow></msub><mo>=</mo><mfrac><mrow><munderover><mo data-mjx-texclass=\"OP\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>i</mi><mo stretchy=\"false\">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mrow><mi>i</mi></mrow></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mrow><mi>i</mi></mrow></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mrow><mi>t</mi></mrow></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mrow><mi>t</mi></mrow></msub></mrow><mrow><munderover><mo data-mjx-texclass=\"OP\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>i</mi><mo stretchy=\"false\">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mrow><mi>i</mi></mrow></msub></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mrow><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac><mo>.</mo><mstyle scriptlevel=\"0\"><mspace width=\"1em\"></mspace></mstyle><mo stretchy=\"false\">(</mo><mn>14</mn><mo stretchy=\"false\">)</mo></math><br>\nWKV 这块（公式 14）是 Time Mix 的核心，它的作用就是前面提到的 RWKV 的 &quot;注意力&quot; 机制。 WKV 的计算有点类似于 Softmax，但是在分子和分母上分别加了一个当前位置 V_t 的项。 和注意力公式差不多，WKV_t 可以理解为：位置 t 相对于它之前各个位置的相关性（注意力程度），WKV_t 是一个大小为 C 的向量（C 是 channel 数）。R、K、V 的计算和 Transformer 的区别是，作为计算 RKV（QKV）的输入的 x 不再是当前 token 的 Embedding，而是当前 token 与上一个 token embedding 的加权和。</p>\n<p>最后， Receptance 和 WKV_t 相乘（Element-Wise Product，两边都是大小为 C 的向量），得到位置 t 的输出 ot （也是大小为 C 的向量）。显然，<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>o</mi><mi>t</mi></msub><mo separator=\"true\">;</mo><mtext>的计算包含了历史信息，随着</mtext><mi>t</mi><mtext>的增加，</mtext></mrow><annotation encoding=\"application/x-tex\">o_t; 的计算包含了历史信息，随着 t 的增加，</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">o</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord cjk_fallback\">的</span><span class=\"mord cjk_fallback\">计</span><span class=\"mord cjk_fallback\">算</span><span class=\"mord cjk_fallback\">包</span><span class=\"mord cjk_fallback\">含</span><span class=\"mord cjk_fallback\">了</span><span class=\"mord cjk_fallback\">历</span><span class=\"mord cjk_fallback\">史</span><span class=\"mord cjk_fallback\">信</span><span class=\"mord cjk_fallback\">息</span><span class=\"mord cjk_fallback\">，</span><span class=\"mord cjk_fallback\">随</span><span class=\"mord cjk_fallback\">着</span><span class=\"mord mathnormal\">t</span><span class=\"mord cjk_fallback\">的</span><span class=\"mord cjk_fallback\">增</span><span class=\"mord cjk_fallback\">加</span><span class=\"mord cjk_fallback\">，</span></span></span></span>o_t; 会依赖于越来越长的历史。</p>\n<ul>\n<li>RWKV 的优点：结合了 Transformer 和 RNN 的优势，训练时能够像 Transformer 那样并行计算，推理时又能像 RNN 那样高效。尤其是后者，对于降低模型成本，尤其是在端侧部署有重要意义。另外 RWKV 的计算量与上下文长度无关，对于更长的上下文可能有更好的扩展性。</li>\n<li>RWKV 的缺点：和 RNN 一样，历史信息是靠隐状态（WKV）来记忆的，对于长距离历史信息的记忆不如 Transformer。这个很容易理解，因为 RWKV 的历史信息是存在一个向量里，时间越久衰减就越厉害，与 Full Attention 比自然是有局限性的。这个局限性也使得 Prompt Engineering 对 RWVK 更加重要。与 Transformer 相比，由于 RWKV 对很长的上下文记忆能力有限，如何设计提示对模型的性能会有很大影响。</li>\n</ul>\n<h4>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h4>\n<p>Mamba 的架构主要基于 S4 (Structured State Spaces for Sequence Modeling)，这是一种最新的状态空间模型 (SSM，State Space  Model) 架构。详细介绍<br>\n可见<a href=\"https://blog.csdn.net/v_JULY_v/article/details/134923301\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">一文通透想颠覆 Transformer 的 Mamba：从 SSM、HiPPO、S4 到 Mamba</a>，内容非常详细全面，这里不再赘叙，同时也推荐一下作者七月。</p>\n<p>Mamba 优点：</p>\n<ul>\n<li>改进 transformer 不擅长处理超长的序列的问题，随上下文长度的增加实现线性扩展。</li>\n<li>快速训练和推理。在训练过程中，计算量和内存与序列长度成线性关系，而在推理过程中，由于不需要缓存以前的元素，自回归展开模型每一步只需要恒定的时间。</li>\n</ul>\n<p>Mamba 缺点：</p>\n<ul>\n<li>结构化 SSM 最初被定义为连续系统的离散化版本，对于连续时间数据（如音频、视频）具有较强的归纳偏差。</li>\n<li>选择机制克服了结构化 SSM 在文本和 DNA 等离散数据模态上的弱点，但反过来可能会影响它们在 LTI (线性时不变) SSM 擅长的数据上的性能。</li>\n<li>基于 Transformer 的基础模型（特别是 LLMs）具有丰富的性质和与预训练模型交互的模式，如微调、适应性、提示、上下文学习等，Mamba 可能不具有相似性质。</li>\n<li>实证评估局限于小型模型规模，在大多数强大的开源 LLMs（如 Llama）以及其他循环模型（如 RWKV 和 RetNet）的阈值以下。评估 Mamba 在这些较大规模上是否仍然有利尚待评估。</li>\n</ul>\n<h4>KAN: Kolmogorov-Arnold Networks</h4>\n<p>KAN 网络结构思路来自 Kolmogorov-Arnold 表示定理。MLP 在节点（“神经元”）上具有固定的激活函数，而 KAN 在边（“权重”）上具有可学习的激活函数。在数据拟合和 PDE 求解中，较小的 KAN 可以比较大的 MLP 获得更好的准确性。</p>\n<p>KAN 本质上是样条（Spline）曲线和 MLP 的组合，吸收了两者的优点。即<strong>KAN = MLP + Spline</strong>（在数学中，样条曲线是由多项式分段定义的函数。一般的 Spline 可以是特定区间的 3 阶多项式。在插值问题中，样条插值通常优于多项式插值。）<br>\n&lt;img src=&quot;/image/KAN1.png alt=“多层感知器（MLP）与 Kolmogorov-Arnold 网络（KANs）”&gt;<br>\n 与传统的 MLP 相比，KAN 有 4 个主要特点：</p>\n<ul>\n<li>激活函数位于 &quot;边&quot; 而不是节点（Node）上。</li>\n<li>激活函数是可学习的而不是固定的。</li>\n<li>可以使用非线性核函数来替代 MLP&quot;边&quot;（Edge）上的线性函数。</li>\n<li>可以设定细粒度的结点（Knot）来提高逼近精度。</li>\n</ul>\n<p>KAN 优点：</p>\n<ul>\n<li>使用非线性算子（典型的是样条）可以更快的逼近任意函数。</li>\n<li>精度高</li>\n</ul>\n<p>KAN 缺点：</p>\n<ul>\n<li>训练代价大，训练速度慢（KAN 通常比 MLP 慢 10 倍）。</li>\n</ul>\n<p><a href=\"https://github.com/cheng-haha/\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">此处</a>使用 MNIST 数据集测试 MLP 和 KAN，感兴趣的小伙伴可以试一下 (亲测可用)</p>\n<h4>TTT：Learning to (Learn at Test Time): RNNs with Expressive Hidden States</h4>\nTTT 层作为一种新的信息压缩和模型记忆机制，可以简单地直接替代 Transformer 中的自注意力层。关键思想是使隐藏状态本身成为机器学习模型，而更新规则则成为自监督学习的步骤。通俗来说，在以前，习惯的做法是把某个函数的输出当作Hidden State，通常是一个向量（如RNN）或者很多向量（如Transformer）。但TTT把自己(模型)当作了Hidden State。而这个hidden state的更新是靠一次基于反向传播的更新。\n<img src=\"/image/TTT1.jpg\" alt=\"RNN、注意力、TTT对比\">\n<strong>序列模型会把历史上下文存储在一个隐藏状态中，这是不可避免的。</strong>像Mamba这样的RNN层，会随着时间的推移压缩成一个固定大小的状态，它们虽然效率很高，但性能受限于其表达能力。对于Transformer中的注意力机制，其有一个KV缓存，它会随着时间的推移不断增长。这个状态不会压缩任何历史上下文，但随着上下文长度的增加，成本也会越来越高。因此，TTT架构另辟蹊径，把上下文压缩到模型的权重中。与 Transformer 的Hidden State不同（每读一个token，就完整地扫描整个历史进行查找），TTT 不会随着处理数据的增多而无限扩展。相反，它会将数据编码成一组代表性的变量，即\"权重\"。这种形式是 TTT 模型高性能的原因，无论 TTT 模型处理多少数据，其内部模型的大小都不会改变。\n<p>TTT 优点：</p>\n<ul>\n<li>高效率。</li>\n</ul>\n<p>TTT 缺点：</p>\n<ul>\n<li>TTT 架构的验证案例相对较少，主要集中在特定任务和小规模实验中，尚未在大规模实际应用中得到广泛验证。</li>\n</ul>\n<p><strong>最后，我想说 TTT 的本质依然是 RNN，并且从算法的角度看，这种用空间复杂度（这种说法其实不恰当）置换时间复杂度（长上下文查找效率）的方式导致的结果就是表达能力差，直观的理解就是对于几万的 token 使用的模型参数假设是 1MB，但这 1MB 参数真的能表达几万亿的 token 内容吗，这也是作者只在 1.2B 参数内进行实验的原因吧。</strong></p>\n<p>总结：喜大普奔，大家又可以水论文了！T_T</p>\n",
            "tags": [
                "RWKV",
                "Mamba",
                "KAN",
                "TTT",
                "线性架构"
            ]
        },
        {
            "id": "https://liuliai.github.io/2024/07/01/RCL%E5%AE%9E%E9%AA%8C%E5%AE%A4Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C/",
            "url": "https://liuliai.github.io/2024/07/01/RCL%E5%AE%9E%E9%AA%8C%E5%AE%A4Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C/",
            "title": "RCL实验室Linux服务器操作手册",
            "date_published": "2024-07-01T03:59:00.000Z",
            "content_html": "<h2>RCL实验室Linux服务器操作手册</h2>\n仅供东北大学机器人科学与工程学院<strong>机器人认知实验室（Robot Cognition Lab，RCL）</strong>成员使用。\n<p>RCL 实验室官网见<a href=\"http://neurcl.cn/index.html\" target=\"_blank\" rel=\"noopener\"><strong><font color=\"LimeGreen\">此处</font></strong></a>，感兴趣获取更多详细信息。</p>\n<h4>账号申请方法</h4>\n<p>联系目前服务器管理员 <a href=\"http://tencent://message/?uin=791037420&amp;Site=https://liuliai.github.io/&amp;Menu=yes\" target=\"_blank\" rel=\"noopener\"><strong><font color=\"LimeGreen\">QQ</font></strong></a></p>\n<h4>服务器介绍</h4>\n<h5>服务器IP地址：172.17.27.232</h5>\n<table>\n  <tr>\n    <th align=\"center\">Device</th>\n    <th align=\"center\">Parameter</th>\n  </tr>\n  <tr>\n    <td align=\"center\">CPU</td>\n    <td align=\"center\">2*Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz</td>\n  </tr>\n  <tr>\n    <td align=\"center\">GPU</td>\n    <td align=\"center\">8*NVIDIA GeForce RTX 3090</td>\n  </tr>\n  <tr>\n    <td rowspan=\"2\">ROM</td>\n    <td align=\"center\">2*Intel® SSD D3-S4510 Series 480GB</td>\n  </tr>\n  <tr>\n    <td align=\"center\">2*Intel® SSD D3-S4510 Series 3.84TB</td>\n  </tr>\n</table>\n<ul>\n<li>所有存储设备均采用 RAID 1 镜像配置，确保数据冗余与安全性</li>\n<li>操作系统版本为 Ubuntu 18.04.6 LTS</li>\n<li>CUDA 版本为 12.1</li>\n</ul>\n<h5>服务器IP地址：172.17.27.194</h5>\n<table border=\"1\">\n  <tr>\n    <th align=\"center\">Device</th>\n    <th align=\"center\">Parameter</th>\n  </tr>\n  <tr>\n    <td align=\"center\">CPU</td>\n    <td align=\"center\">2*Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz</td>\n  </tr>\n  <tr>\n    <td align=\"center\">GPU</td>\n    <td align=\"center\">8*NVIDIA GeForce RTX 3090</td>\n  </tr>\n  <tr>\n    <td align=\"center\" rowspan=\"3\">ROM</td>\n    <td align=\"center\">2*Intel® SSD D3-S4510 Series 480GB</td>\n  </tr>\n  <tr>\n    <td align=\"center\">2*Intel® SSD D3-S4510 Series 3.84TB</td>\n  </tr>\n  <tr>\n    <td align=\"center\">2*致态 SC001 XT SATA SSD 2TB</td>\n  </tr>\n</table>\n<ul>\n<li>致态 SC001 XT SATA SSD 2TB 采用 RAID 0 条带化配置，优化读写性能</li>\n<li>其余存储设备采用 RAID 1 镜像配置，确保数据冗余与安全性</li>\n<li>操作系统版本为 Ubuntu 18.04.6 LTS</li>\n<li>CUDA 版本为 11.8</li>\n</ul>\n<h4>登陆方法</h4>\n目前是校内分配的ip，校外是无法访问的。\n<p>①使用校园网 (每个月免费 10G，凌晨 0:00-6:00 免费)，校园网具体资费标准见 <a href=\"https://ipgw.neu.edu.cn\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">此处</a>。</p>\n<p>②使用 OpenVPN，使用学号和手机号验证码登陆，详细介绍在 <a href=\"http://xwb.neu.edu.cn/10185/list.htm\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">此处</a>。</p>\n<h5>命令行使用</h5>\n在 Windows 系统中，使用\"win+R\"快捷键，输入\"cmd\"，然后回车输入ssh user@hostname 或者ssh -l user hostname ，输入密码即可。\n<p>ssh 登录服务器的命令如下。ssh 默认连接服务器的 22 端口，-p 参数可以指定其他端口。<br>\n <code>$ ssh hostname </code> <br>\n上面命令中，hostname 是主机名，它可以是域名，也可能是 IP 地址或局域网内部的主机名（对于 RCL 实验室，为 172.17.27.194 或者 172.17.27.232）。不指定用户名的情况下，将使用客户端的当前用户名，作为远程服务器的登录用户名。如果要指定用户名，可以采用下面的命令。<br>\n <code>$ ssh user@hostname </code> <br>\n上面的命令中，用户名和主机名写在一起了，之间使用 @分隔。用户名也可以使用 ssh 的 - l 参数指定，这样的话，用户名和主机名就不用写在一起了。<br>\n <code>$ ssh -l user hostname </code></p>\n<h5>图形界面使用</h5>\n首先下载VNC，在此处<a href=\"https://www.realvnc.com/en/connect/download/viewer/windows/\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">官网</a>下载，或者<a href=\"https://pan.baidu.com/s/1qyTUfb-U2HPKqmGyg_GupQ?pwd=qn4o\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">百度网盘</a>。\n<p>然后输入 IP 地址和端口号 (端口号为最后两位，以 63 为例)<br>\n<img src=\"/image/RCL-linux-fig1.jpg\" alt=\"\"><br>\n 点击 continue<br>\n<img src=\"/image/RCL-linux-fig2.jpg\" alt=\"\"><br>\n 输入默认密码 123456 即可<br>\n<img src=\"/image/RCL-linux-fig3.jpg\" alt=\"\"></p>\n<h4>传输文件</h4>\n①在本地cmd上，可以直接通过scp命令传输文件和文件夹。\n<p>从本地上传到服务器，如果想要把文件夹整个上传到服务器，那么加上参数 - r 即可。</p>\n<p><code>$ scp [-r] {local_file_path} {user}@{hostname}:{remote_file_path}</code></p>\n<p>从远程下载到本地</p>\n<p><code>$ scp [-r] {user}@{hostname}:{remote_file_path} {local_file_path}</code></p>\n<p>详细使用信息见  <a href=\"https://www.runoob.com/linux/linux-comm-scp.html\" target=\"_blank\" rel=\"noopener\"><strong><font color=\"LimeGreen\">此处</font></strong></a></p>\n<p>②使用 ftp 工具 WinSCP 或者 FileZilla<br>\n 参考教程 <a href=\"https://blog.csdn.net/qq_26383975/article/details/120220823\" target=\"_blank\" rel=\"noopener\"><strong><font color=\"LimeGreen\">文件传输工具 WinSCP 下载安装教程</font></strong></a>和<a href=\"https://blog.csdn.net/weixin_45309916/article/details/107782070\" target=\"_blank\" rel=\"noopener\"><strong><font color=\"LimeGreen\">FileZilla 的下载与安装以及简单使用（有图解超简单）</font></strong></a></p>\n<h4>文件系统介绍</h4>\n目前每人的账号默认分配在home中（容量较小），请将数据集，conda环境创建在data(自动备份)或者bigdata(有需要请自行备份)中进行。\n<p><strong>如有疑问请评论。</strong></p>\n",
            "tags": [
                "Linux",
                "教程",
                "服务器"
            ]
        },
        {
            "id": "https://liuliai.github.io/2024/06/15/Linux%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/",
            "url": "https://liuliai.github.io/2024/06/15/Linux%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/",
            "title": "Linux 常用命令简单介绍",
            "date_published": "2024-06-15T03:59:00.000Z",
            "content_html": "<p>##Linux 常用命令简单介绍<br>\n与 Windows 的直观图形界面相比，Linux 的命令行界面显得有些抽象，尤其是对于初涉 Linux 的朋友，每当遇到新的 Linux 命令，我们通常会花费时间在搜索引擎上查找相关的使用说明。这样的过程既耗时又繁琐。因此，我整理了一些 Linux 的基础命令，作为我学习 Linux 过程中的一个备忘录。同时也希望能够为那些想要深入探索 Linux 世界的读者提供一些帮助。</p>\n<pre><code>def nextGreaterElement(self, nums1: List[int], nums2: List[int]) -&gt; List[int]:\n    idx &#x3D; {x: i for i, x in enumerate(nums1)}\n    ans &#x3D; [-1] * len(nums1)\n    st &#x3D; []\n    for x in reversed(nums2):\n        while st and x &gt;&#x3D; st[-1]:\n            # 由于 x 的出现，栈顶元素永远不会是左边元素的「下一个更大元素」\n            st.pop()\n        if st and x in idx:  # x 在 nums1 中\n            ans[idx[x]] &#x3D; st[-1]  # 记录答案\n        st.append(x)\n    return ans</code></pre>\n<h4 id=\"apt-get-ubuntu系统包管理器\"><a class=\"markdownIt-Anchor\" href=\"#apt-get-ubuntu系统包管理器\">#</a> apt-get   Ubuntu 系统包管理器</h4>\n<h5 id=\"命令\"><a class=\"markdownIt-Anchor\" href=\"#命令\">#</a> 命令:</h5>\n<p>update - 检索 新的包列表<br>\n upgrade - 升级 可更新的所有软件包<br>\n install - 安装 新软件包（pkg 是 libc6 不是 libc6.deb）<br>\nremove - 删除 软件包<br>\n autoremove - 自动删除 所有未使用的软件包<br>\n purge - 删除 软件包和配置文件<br>\n clean - 清除 已下载的归档文件<br>\n autoclean - 清除 旧的下载的档案文件<br>\n check - 验证 是否有损坏的依赖<br>\n download - 下载 二进制包到当前目录</p>\n<h5 id=\"选项\"><a class=\"markdownIt-Anchor\" href=\"#选项\">#</a> 选项：</h5>\n<p>-q ：不输出任何信息<br>\n - qq ：除了错误之外，没有输出<br>\n - d ：仅下载，不要安装或解压缩存档<br>\n - y ：对所有确定询问都选择 Yes，并且不提示<br>\n - f ：尝试纠正 被破坏依赖关系的系统<br>\n - m ：如果存档是可定位的，则尝试继续<br>\n - u ：显示升级包的列表<br>\n - b ：在获取源代码包后构建源包</p>\n<h6 id=\"检索-新的包列表\"><a class=\"markdownIt-Anchor\" href=\"#检索-新的包列表\">#</a> 检索 新的包列表</h6>\n<p>apt-get update</p>\n<h6 id=\"升级-可更新的所有软件包注意这个命令会升级所有的软件包所以会升级很长时间\"><a class=\"markdownIt-Anchor\" href=\"#升级-可更新的所有软件包注意这个命令会升级所有的软件包所以会升级很长时间\">#</a> 升级 可更新的所有软件包（注意这个命令会升级所有的软件包，所以会升级很长时间）</h6>\n<p>apt-get upgrade</p>\n<h6 id=\"安装-nginx-软件包\"><a class=\"markdownIt-Anchor\" href=\"#安装-nginx-软件包\">#</a> 安装 Nginx 软件包</h6>\n<p>apt-get install nginx</p>\n<h6 id=\"卸载-nginx-软件包\"><a class=\"markdownIt-Anchor\" href=\"#卸载-nginx-软件包\">#</a> 卸载 Nginx 软件包</h6>\n<p>apt-get remove nginx</p>\n<h6 id=\"卸载-nginx-软件包-并删除所有相关配置文件\"><a class=\"markdownIt-Anchor\" href=\"#卸载-nginx-软件包-并删除所有相关配置文件\">#</a> 卸载 Nginx 软件包 并删除所有相关配置文件</h6>\n<p>apt-get remove --purge nginx</p>\n<h6 id=\"在安装软件和卸载的时候为了避免误操作都会询问是否继续每次都要输入-y-来确定会很麻烦可以加上-y-参数\"><a class=\"markdownIt-Anchor\" href=\"#在安装软件和卸载的时候为了避免误操作都会询问是否继续每次都要输入-y-来确定会很麻烦可以加上-y-参数\">#</a> 在安装软件和卸载的时候，为了避免误操作，都会询问是否继续，每次都要输入 y 来确定会很麻烦，可以加上 -y 参数</h6>\n<h6 id=\"安装-nginx-软件包-并不显示确定提示\"><a class=\"markdownIt-Anchor\" href=\"#安装-nginx-软件包-并不显示确定提示\">#</a> 安装 Nginx 软件包 并不显示确定提示</h6>\n<p>apt-get install nginx -y</p>\n<h6 id=\"卸载-nginx-软件包删除所有相关配置文件-并不显示提示\"><a class=\"markdownIt-Anchor\" href=\"#卸载-nginx-软件包删除所有相关配置文件-并不显示提示\">#</a> 卸载 Nginx 软件包，删除所有相关配置文件 并不显示提示</h6>\n<p>apt-get remove --purge nginx -y</p>\n<h6 id=\"清除-旧的无用-的软件包\"><a class=\"markdownIt-Anchor\" href=\"#清除-旧的无用-的软件包\">#</a> 清除 旧的 / 无用 的软件包</h6>\n<p>apt-get clean &amp;&amp; apt-get autoclean</p>\n<h6 id=\"下载-nginx-二进制软件包到当前目录但不解压和安装\"><a class=\"markdownIt-Anchor\" href=\"#下载-nginx-二进制软件包到当前目录但不解压和安装\">#</a> 下载 Nginx 二进制软件包到当前目录，但不解压和安装</h6>\n<p>apt-get download nginx -d</p>\n<h6 id=\"更多的命令可以用-apt-get-help-查看\"><a class=\"markdownIt-Anchor\" href=\"#更多的命令可以用-apt-get-help-查看\">#</a> 更多的命令可以用 apt-get --help 查看。</h6>\n<h6 id=\"mkdir-新建文件夹\"><a class=\"markdownIt-Anchor\" href=\"#mkdir-新建文件夹\">#</a> mkdir 新建文件夹</h6>\n<h6 id=\"在当前文件夹新建一个-bash-文件夹\"><a class=\"markdownIt-Anchor\" href=\"#在当前文件夹新建一个-bash-文件夹\">#</a> 在当前文件夹新建一个 bash 文件夹</h6>\n<p>mkdir bash</p>\n<h6 id=\"更多的命令可以用-mkdir-help-查看\"><a class=\"markdownIt-Anchor\" href=\"#更多的命令可以用-mkdir-help-查看\">#</a> 更多的命令可以用 mkdir --help 查看。</h6>\n<h5>cd 进入文件夹</h5>\n###### 一般结合ll(小写L)命令使用(全称：ls -l)，输出当前目录下的所有目录，然后使用cd命令跳转到某个目录\n###### 例如当前为起始/root目录中 ，跳转到 /root/data\ncd data\n<h6 id=\"也可以多次跳转输入目录的绝对路径因此也可用作返回目录\"><a class=\"markdownIt-Anchor\" href=\"#也可以多次跳转输入目录的绝对路径因此也可用作返回目录\">#</a> 也可以多次跳转，输入目录的绝对路径（因此也可用作返回目录）</h6>\n<p>cd /root/data/user/models/</p>\n<h6 id=\"值得注意的是在windows中的cmd使用cd命令时切换不同磁盘的目录时需要额外增加-d\"><a class=\"markdownIt-Anchor\" href=\"#值得注意的是在windows中的cmd使用cd命令时切换不同磁盘的目录时需要额外增加-d\">#</a> 值得注意的是，在 Windows 中的 CMD 使用 cd 命令时，切换不同磁盘的目录时，需要额外增加 /d</h6>\n<p>cd /d D:\\Program Files\\PycharmProjects\\experiment</p>\n<h6 id=\"返回上级目录\"><a class=\"markdownIt-Anchor\" href=\"#返回上级目录\">#</a> 返回上级目录</h6>\n<p>cd …</p>\n<h5>cp 复制或重命名文件/文件夹</h5>\n###### 复制当前目录内的 log.txt文件到 /var目录\ncp log.txt /var/log.txt\n<h6 id=\"复制当前目录内的-bash文件夹到-home目录\"><a class=\"markdownIt-Anchor\" href=\"#复制当前目录内的-bash文件夹到-home目录\">#</a> 复制当前目录内的 bash 文件夹到 /home 目录</h6>\n<p>cp -R bash /home/bash</p>\n<h6 id=\"复制当前目录内的所有txt后缀的文件到-varlog目录\"><a class=\"markdownIt-Anchor\" href=\"#复制当前目录内的所有txt后缀的文件到-varlog目录\">#</a> 复制当前目录内的所有.txt 后缀的文件到 /var/log 目录</h6>\n<p>cp *.txt /var/log</p>\n<h6 id=\"复制当前目录内的所有以-sy1a-5uva-4q3n开头的文件到-varlog目录\"><a class=\"markdownIt-Anchor\" href=\"#复制当前目录内的所有以-sy1a-5uva-4q3n开头的文件到-varlog目录\">#</a> 复制当前目录内的所有以 SY1A-5UVA-4Q3N 开头的文件到 /var/log 目录</h6>\n<p>cp SY1A-5UVA-4Q3N* /var/log</p>\n<h6 id=\"复制当前目录内的所有以-sy1a-5uva-4q3n开头-以txt后缀结尾的文件到-varlog目录\"><a class=\"markdownIt-Anchor\" href=\"#复制当前目录内的所有以-sy1a-5uva-4q3n开头-以txt后缀结尾的文件到-varlog目录\">#</a> 复制当前目录内的所有以 SY1A-5UVA-4Q3N 开头 以.txt 后缀结尾的文件到 /var/log 目录</h6>\n<p>cp SY1A-5UVA-4Q3N*.txt /var/log</p>\n<h6 id=\"假设当前目录是-rootuserlog要把这个目录中的所有txt后缀的文件复制到上一级目录-rootuser那么这样做\"><a class=\"markdownIt-Anchor\" href=\"#假设当前目录是-rootuserlog要把这个目录中的所有txt后缀的文件复制到上一级目录-rootuser那么这样做\">#</a> 假设当前目录是 /root/user/log，要把这个目录中的所有.txt 后缀的文件复制到上一级目录 /root/user，那么这样做</h6>\n<p>cp *.txt …</p>\n<h6 id=\"就是相对路径代表上一级目录当然你也可以用绝对路径这样更不容易出错\"><a class=\"markdownIt-Anchor\" href=\"#就是相对路径代表上一级目录当然你也可以用绝对路径这样更不容易出错\">#</a> … 就是相对路径，代表上一级目录，当然你也可以用绝对路径，这样更不容易出错</h6>\n<p>cp *.txt /root/user</p>\n<h6 id=\"重命名当前目录内的-logtxt文件为-log2txt\"><a class=\"markdownIt-Anchor\" href=\"#重命名当前目录内的-logtxt文件为-log2txt\">#</a> 重命名当前目录内的 log.txt 文件为 log2.txt</h6>\n<p>cp log.txt log2.txt</p>\n<h6 id=\"复制当前目录内的-logtxt文件到-var目录并重命名为-log1txt\"><a class=\"markdownIt-Anchor\" href=\"#复制当前目录内的-logtxt文件到-var目录并重命名为-log1txt\">#</a> 复制当前目录内的 log.txt 文件到 /var 目录并重命名为 log1.txt</h6>\n<p>cp log.txt /var/log1.txt</p>\n<h6 id=\"复制当前目录内的-bash文件夹到-home目录并重命名为-bash2\"><a class=\"markdownIt-Anchor\" href=\"#复制当前目录内的-bash文件夹到-home目录并重命名为-bash2\">#</a> 复制当前目录内的 bash 文件夹到 /home 目录并重命名为 bash2</h6>\n<p>cp -R bash /home/bash2</p>\n<h6 id=\"复制当前目录内的-logtxt文件到-var目录但是-var-目录中已经存着-logtxt那么会提示-cp-overwrite-varlogtxt-可以用-f-强制覆盖\"><a class=\"markdownIt-Anchor\" href=\"#复制当前目录内的-logtxt文件到-var目录但是-var-目录中已经存着-logtxt那么会提示-cp-overwrite-varlogtxt-可以用-f-强制覆盖\">#</a> 复制当前目录内的 log.txt 文件到 /var 目录，但是 /var 目录中已经存着 log.txt，那么会提示 cp: overwrite `/var/log.txt’? 可以用 -f 强制覆盖</h6>\n<p>cp -f log /var/log.txt</p>\n<h6 id=\"大家可能会发现当你使用-cp-f-强制覆盖的时候依然会询问你是否覆盖这是因为-cp-为了避免你手误默认加上了-i-参数该参数代表每次覆盖必须询问\"><a class=\"markdownIt-Anchor\" href=\"#大家可能会发现当你使用-cp-f-强制覆盖的时候依然会询问你是否覆盖这是因为-cp-为了避免你手误默认加上了-i-参数该参数代表每次覆盖必须询问\">#</a> 大家可能会发现，当你使用 cp -f 强制覆盖的时候，依然会询问你是否覆盖，这是因为 CP 为了避免你手误，默认加上了 -i 参数（该参数代表每次覆盖必须询问）。</h6>\n<h6 id=\"所以想要避免-cp-默认的-i-参数只需要在-cp-命令前面加上斜杠即可\"><a class=\"markdownIt-Anchor\" href=\"#所以想要避免-cp-默认的-i-参数只需要在-cp-命令前面加上斜杠即可\">#</a> 所以想要避免 CP 默认的 -i 参数，只需要在 CP 命令前面加上斜杠即可 “/”</h6>\n<p>/cp -f log /var/log.txt</p>\n<p>####### 复制当前目录内的 log.txt log1.txt log2.txt 文件和 log233 目录到 /home/log 目录中<br>\n cp -R log.txt log1.txt log2.txt log233 /home/log</p>\n<p>####### 更多的命令可以用 cp --help 查看。</p>\n<h5>mv 移动或重命名文件/文件夹</h5>\n####### 移动当前目录内的 log.txt文件到 /var目录\nmv log.txt /var/log.txt\n<p>####### 移动当前目录内的 bash 文件夹到 /home 目录<br>\n mv bash /home/bash</p>\n<p>####### 重命名当前目录内的 log.txt 文件为 log2.txt<br>\nmv log.txt log2.txt</p>\n<p>####### 复制当前目录内的 log.txt 文件到 /var 目录并重命名为 log1.txt<br>\nmv log.txt /var/log1.txt</p>\n<p>####### 复制当前目录内的 bash 文件夹到 /home 目录并重命名为 bash2<br>\nmv bash /home/bash2</p>\n<p>####### 更多的命令可以用 mv --help 查看。</p>\n<h5>rm 删除文件/文件夹</h5>\n####### 删除当前目录下的 log.txt文件\nrm log.txt\n<p>####### 删除当前目录下所有.txt 后缀的文件<br>\n rm *.txt</p>\n<p>####### 使用 rm 命令删除时，会提示你是否确定删除，输入 y 即删除，输入 n 则取消<br>\n ####### rm: remove regular file `log.txt’? y</p>\n<p>####### 删除当前目录下所有.txt 后缀的文件<br>\n rm *.txt</p>\n<p>####### 删除当前目录下所有以 SY1A-5UVA-4Q3N 开头的文件<br>\n rm SY1A-5UVA-4Q3N*</p>\n<p>####### 删除当前目录下所有以 SY1A-5UVA-4Q3N 开头 以.txt 后缀结尾的文件<br>\n rm SY1A-5UVA-4Q3N*.txt</p>\n<p>####### 当你用 rm 删除目录的时候会发现提示这不是一个文件<br>\n ####### rm bash<br>\n####### rm: cannot remove `bash’: Is a directory<br>\n####### 可以加上 -r 来归递删除目录及其目录下的内容<br>\n rm -r bash</p>\n<p>####### 因为为了避免手误删除错误，所以 rm 默认是加上了 -i 的参数，也就是每一次删除文件 / 目录都会提示，如果觉得烦可以用 -rf 参数<br>\n rm -rf bash</p>\n<p>####### rm -rf 这个命令请慎重使用，而且千万不要使用 rm -rf / 或者 rm -rf /* 之类的命令 (系统自杀)，可能会让你系统爆炸，所以使用请慎重！</p>\n<p>####### 更多的命令可以用 rm --help 查看。</p>\n<h5>ls 显示目录中文件（l为小写L）</h5>\n####### 显示当前目录下的所有文件, -a代表“all”，这个命令会列出目录中的所有文件，包括以点（.）开头的隐藏文件。它不会显示文件的详细信息，只会显示文件名。\nls -a\n<p>####### 它不仅显示文件名，还显示文件的权限、所有者、大小、最后修改日期等详细信息。<br>\nls -l</p>\n<p>####### 命令后面加上 绝对路径 / 相对路径 就会显示指定文件夹内的所有文件<br>\n ls -a bash/log</p>\n<p>####### 相对路径，当前目录是 /root ，欲查看的目录是 /root/bash/log<br>\nls -a /root/bash/log</p>\n<p>####### 绝对路径， 当前目录是 /root ，欲查看的目录是 /root/bash/log</p>\n<p>####### 更多的命令可以用 ls --help 来查看。</p>\n<h5>du 查看文件/文件夹占用磁盘空间的大小</h5>\n参数介绍：\n-h ：以人类易读的方式显示\n-a ：显示 目录占用的磁盘空间大小，并显示其下目录和文件占用磁盘空间的大小\n-s ：显示 目录占用的磁盘空间大小，但不显示其下子目录和文件占用的磁盘空间大小\n-c ：显示几个目录或文件占用的磁盘空间大小，还要统计它们的总和\n--apparent-size：显示目录或文件自身的大小\n-l ：统计硬链接占用磁盘空间的大小\n-L ：统计符号链接所指向的文件占用的磁盘空间大小\n<p>####### 假设当前位于 /root 目录下，则显示 /root 文件夹的大小，但不显示其子目录和文件的大小<br>\n du -sh</p>\n<p>####### 假设当前位于 /root 目录下，则显示 /root 文件夹的大小，并显示其子目录和文件的大小<br>\n du -ah</p>\n<p>####### 假设当前位于 /root 目录下，则显示 /root 文件夹下的所有文件夹的大小及其总和<br>\n du -lh --max-depth=1</p>\n<p>####### 更多的命令可以用 du --help 来查看。</p>\n<h5>cat 查看文件内容</h5>\n假设 log.txt文件的内容为：\n<p>SY1A-5UVA-4Q3N233<br>\nSY1A-5UVA-4Q3N</p>\n<p>SY1A-5UVA-4Q3N666</p>\n<p>SY1A-5UVA-4Q3N2366<br>\nSY1A-5UVA-4Q3N8888<br>\n 查看文件：</p>\n<p>####### 查看 log.txt 文件的所有内容<br>\n cat log.txt<br>\n####### 输出示例如下<br>\n SY1A-5UVA-4Q3N233<br>\nSY1A-5UVA-4Q3N</p>\n<p>SY1A-5UVA-4Q3N666</p>\n<p>SY1A-5UVA-4Q3N2366<br>\nSY1A-5UVA-4Q3N8888</p>\n<p>####### 查看 log.txt 文件的所有内容，并对所有行编号<br>\n cat -n log.txt<br>\n####### 输出示例如下：<br>\n1\tSY1A-5UVA-4Q3N233<br>\n2\tSY1A-5UVA-4Q3N<br>\n3\t<br>\n4\t<br>\n5\tSY1A-5UVA-4Q3N666<br>\n6\t<br>\n7\tSY1A-5UVA-4Q3N2366<br>\n8\tSY1A-5UVA-4Q3N8888</p>\n<p>####### 查看 log.txt 文件的所有内容，并对非空行编号<br>\n cat -b log.txt<br>\n####### 输出示例如下：<br>\n1\tSY1A-5UVA-4Q3N233<br>\n2\tSY1A-5UVA-4Q3N</p>\n<pre><code> 3\tSY1A-5UVA-4Q3N666\n\n 4\tSY1A-5UVA-4Q3N2366\n 5\tSY1A-5UVA-4Q3N8888\n</code></pre>\n<p>####### 查看 log.txt 文件的所有内容，并对非空行编号，且不输出多行空行<br>\n cat -bs log.txt<br>\n####### 输出示例如下：<br>\n1\tSY1A-5UVA-4Q3N233<br>\n2\tSY1A-5UVA-4Q3N</p>\n<pre><code> 3\tSY1A-5UVA-4Q3N666\n\n 4\tSY1A-5UVA-4Q3N2366\n 5\tSY1A-5UVA-4Q3N8888\n</code></pre>\n<p>清空文件：</p>\n<p>####### 清空当前目录中的 log.txt 文件<br>\n cat /dev/null &gt; log.txt</p>\n<p>####### 清空 /var 目录中的 log.txt 文件<br>\n cat /dev/null &gt; /var/log.txt<br>\n 写入文件：</p>\n<p>####### 写入文本到当前目录中的 log.txt 文件中 (加入文本到文件内容最后)<br>\ncat &gt;&gt; log.txt &lt;&lt;-EOF<br>\nSY1A-5UVA-4Q3N<br>\nSY1A-5UVA-4Q3N233<br>\nSY1A-5UVA-4Q3N666<br>\nEOF</p>\n<p>####### 清空文件并写入文本到 /var 目录中的 log.txt 文件中 (先清空后写入)<br>\ncat &gt; /var/log.txt &lt;&lt;-EOF<br>\nSY1A-5UVA-4Q3N<br>\nSY1A-5UVA-4Q3N233<br>\nSY1A-5UVA-4Q3N666<br>\nEOF</p>\n<p>####### 更多的命令可以用 cat --help 来查看。</p>\n<h5>VI、VIM 编辑文件内容</h5>\n####### 打开当前目录下的 log.txt文件，如果没有那么会新建 log.txt文件（安装vim后，使用 vi和 vim打开文件没区别）\nvi log.txt\nvim log.txt\n####### 在命令行模式下，直接输入以下 符号和字母(区分大小写)\n#进入编辑模式（插入模式，按 Esc键 即可返回命令行模式）\ni\n####### 删除光标当前所在的一行\ndd\n####### 删除文件内所有内容\ndddG\n####### 复制光标当前所在的一行\nyy\n####### 粘贴刚才复制的一行内容\np\n####### 撤销上个操作（误操作可以用这个恢复）\nu\n####### 保存当前文件（ : 是英文的冒号）\n:w\n####### 另存当前文件内容为 log2.txt\n:w log2.txt\n####### 退出当前文件\n:q\n####### 不保存 并强制退出当前文件\n:q!\n####### 保存并退出当前文件\n:wq\n<p>####### 更多的命令可以用 vi --help /vim --help 来查看。</p>\n<h5>wget 下载工具</h5>\n参数介绍：\n<p>####### 只介绍最常用的参数<br>\n ####### 如果提示命令不存在，那么使用 yum install wget -y /apt-get install wget -y 来安装（有一些非常精简的系统可能会没装）</p>\n<p>-b ：启动后，后台下载<br>\n - q ：安静模式（不输出任何信息）<br>\n-c ：断点续传下载文件<br>\n - O ：指定下载后的文件名（可使用绝对路径目录 + 文件名）<br>\n-P ：指定下载后的文件目录（-P 只能指定下载目录，并不能指定文件名）<br>\n-t ：设置重试次数（0 代表无限）<br>\n-T ：设置超时时间（单位：秒）<br>\n-N ：只获取比本地新的文件（新的覆盖旧的）<br>\n-4 ：仅连接至 IPv4 地址<br>\n - 6 ：仅连接至 IPv6 地址<br>\n–limit-rate=xxxk : 限制下载速度（k 代表 KB/S）<br>\n–post-data ：通过 POST 方式发送数据<br>\n–no-check-certificate ：不验证服务器的 SSL 证书</p>\n<p>####### 下载一个文件到当前目录<br>\n wget <a href=\"https://yun.doubibackup.com/100MB.bin\" target=\"_blank\" rel=\"noopener\">https://yun.doubibackup.com/100MB.bin</a></p>\n<p>####### 下载文件到当前目录并重命名为 200MB.bin<br>\nwget -O “200MB.bin” <a href=\"https://yun.doubibackup.com/100MB.bin\" target=\"_blank\" rel=\"noopener\">https://yun.doubibackup.com/100MB.bin</a></p>\n<p>####### 下载文件到 /root 目录（-P 只能指定下载目录，并不能指定文件名）<br>\nwget -P “/root” <a href=\"https://yun.doubibackup.com/100MB.bin\" target=\"_blank\" rel=\"noopener\">https://yun.doubibackup.com/100MB.bin</a></p>\n<p>####### 下载文件到 /root/doubi 目录并重命名为 200MB.bin<br>\nwget -O “/root/doubi/200MB.bin” <a href=\"https://yun.doubibackup.com/100MB.bin\" target=\"_blank\" rel=\"noopener\">https://yun.doubibackup.com/100MB.bin</a></p>\n<p>####### 下载文件完成之前 wget 进程结束了，那么可以使用断点续传重新下载中断的文件（前提是下载服务器支持断点续传）<br>\nwget -c <a href=\"https://yun.doubibackup.com/100MB.bin\" target=\"_blank\" rel=\"noopener\">https://yun.doubibackup.com/100MB.bin</a></p>\n<p>####### 通过后台下载文件到 /root/doubi 目录并重命名为 200MB.bin<br>\nwget -b -O “/root/doubi/200MB.bin” <a href=\"https://yun.doubibackup.com/100MB.bin\" target=\"_blank\" rel=\"noopener\">https://yun.doubibackup.com/100MB.bin</a><br>\n####### Continuing in background, pid 2333.<br>\n####### Output will be written to `wget-log’.<br>\n####### 后台下后，你可以使用以下命令来查看下载进度：<br>\ntail -f wget-log</p>\n<p>####### 有时候一些 Linux 系统中的 SSL 证书不完整，会导致下载一些 HTTPS 网站文件的时候会验证 SSL 证书失败，可以这样做<br>\n ####### 不验证服务器 SSL 证书，下载文件到当前目录并重命名为 200MB.bin<br>\nwget --no-check-certificate -O “200MB.bin” <a href=\"https://yun.doubibackup.com/100MB.bin\" target=\"_blank\" rel=\"noopener\">https://yun.doubibackup.com/100MB.bin</a></p>\n<p>####### 使用 wget 发送 POST 请求数据<br>\n wget --post-data “user=doubi&amp;passwd=23333” <a href=\"https://xxx.xx/\" target=\"_blank\" rel=\"noopener\">https://xxx.xx/</a></p>\n<p>####### 下载文件到当前目录 并仅通过 IPv4 连接 只获取比本地新的文件，限速 200KB/S<br>\nwget --limit-rate=200k -N -4 <a href=\"https://yun.doubibackup.com/100MB.bin\" target=\"_blank\" rel=\"noopener\">https://yun.doubibackup.com/100MB.bin</a></p>\n<p>####### 下载文件到当前目录 并重试次数为 1，超时时间为 2 秒<br>\n wget -t1 -T2 <a href=\"https://yun.doubibackup.com/100MB.bin\" target=\"_blank\" rel=\"noopener\">https://yun.doubibackup.com/100MB.bin</a></p>\n<p>####### 通过 wget 来获取服务器的外网 IP（-qO- 代表运行完会输出下载的信息，并不会保存到本地文件）<br>\nwget -qO- <a href=\"http://ipinfo.io/ip\" target=\"_blank\" rel=\"noopener\">ipinfo.io/ip</a></p>\n<p>####### 更多的命令可以用 wget --help 来查看。</p>\n",
            "tags": [
                "Linux",
                "教程"
            ]
        },
        {
            "id": "https://liuliai.github.io/2024/04/25/Cantor%EF%BC%9A%20Inspiring%20Multimodal%20Chain-of-Thought%20of%20MLLM/",
            "url": "https://liuliai.github.io/2024/04/25/Cantor%EF%BC%9A%20Inspiring%20Multimodal%20Chain-of-Thought%20of%20MLLM/",
            "title": "Cantor：Inspiring Multimodal Chain-of-Thought of MLLM",
            "date_published": "2024-04-25T03:59:00.000Z",
            "content_html": "<h2>Cantor: Inspiring Multimodal Chain-of-Thought of MLLM</h2>\n<p>本文是论文<a href=\"https://arxiv.org/abs/2404.16033\" target=\"_blank\" rel=\"noopener\"><strong><font color=\"LimeGreen\">Cantor: Inspiring Multimodal Chain-of-Thought of MLLM</font></strong></a>的阅读笔记和个人理解，论文由厦门大学和腾讯优图实验室共同合作而成。探讨了一种多模态 CoT 框架，名为 Cantor，它具有感知决策架构，有效地集成了视觉上下文和逻辑推理来解决视觉推理任务。</p>\n<p>Cantor 的特点是感知 - 决策架构。Cantor 首先充当决策生成器，并集成视觉输入来分析图像和问题，确保与实际上下文更紧密地保持一致。此外，Cantor 利用 MLLM 的高级认知功能作为多方面的专家来获取更高层次的信息，从而增强 CoT 生成过程。</p>\n<p><strong>这种多模态思维链方法无需额外训练</strong></p>\n<p><img src=\"/image/Cantor-fig1.jpg\" alt=\" 决策生成中视觉信息的比较\"></p>\n<h6>（a） 决策生成中视觉信息的比较：由于图像不够清晰，询问GPT-3.5（没有视觉背景）会导致“确定幻觉”。Cantor（带字幕）通过字幕引入视觉背景，不会遇到这个问题。Cantor（带图像）更加精确，提高了任务分配的合理性。（b） 不同视觉工具的比较：传统方法中使用的低级专业感知工具只能获得基本数据。由MLLM代理的高级通用认知专家获得对象数关系，实现直接和后续推理。</h6>\n<p><strong>但我感觉这个图的比较不太恰当，在多模态 CoT 领域，却拿 GPT-3.5 这种 LLM 来做比较，只询问烧杯最大刻度，缺乏图像信息当然无法正确回答了，为什么不使用 GPT-4V 或者 Gemini 等多模态模型进行对比呢，而且 Baseline 就应该使用图像字幕的形式，而不是说 Cantor 通过字幕引入了视觉语境，避免了决策幻觉。</strong></p>\n<blockquote>\n<p>摘要</p>\n</blockquote>\n<p>随着大型语言模型 （LLMs）的出现，通过思维链 （CoT）方法得到增强，视觉推理问题通常被分解为可管理的子任务，并用各种外部工具依次解决。然而，由于视觉信息不足和低级感知工具的局限性，无法提供全面推理所需的抽象摘要，这种范式面临着决策中潜在的 “决定性幻觉” 的挑战。我们认为，融合视觉上下文获取和逻辑推理对于解决视觉推理任务至关重要。本文深入研究了多模态 CoT 领域，以多模态大型语言模型 （MLLMs）及其认知能力解决复杂的视觉推理任务。为此，我们提出了一种创新的多模态 CoT 框架，称为 Cantor，其特点是感知 - 决策架构。Cantor 首先充当决策生成器，并集成视觉输入来分析图像和问题，确保与实际上下文更紧密地保持一致。此外，Cantor 利用 MLLM 的高级认知功能作为多方面的专家来获取更高层次的信息，从而增强 CoT 生成过程。我们广泛的实验证明了所提出的框架的有效性，表明在两个复杂的视觉推理数据集中，多模态 CoT 性能显着提高，而无需微调或地面真实原理。项目页面：<br>\n<a href=\"https://ggg0919.github.io/cantor/\" target=\"_blank\" rel=\"noopener\"><strong><font color=\"HotPink\">https://ggg0919.github.io/cantor/</font></strong></a> .</p>\n<p><img src=\"/image/Cantor-fig2.jpg\" alt=\"Cantor概述和具体示例\"></p>\n<h6>Cantor概述和具体示例。Cantor通过决策生成器分析图像和问题，提供问题的原理分析，并提供模块选择和原因，以及具体的任务分配。随后，MLLM充当各种专家模块来执行子任务。最后，Cantor通过答案生成器进行综合和思考，提供最终答案。</h6>\n<blockquote>\n<p>引言</p>\n</blockquote>\n<p>随着大型语言模型 （LLMs）的发展，研究者开始采用思维链 （CoT）策略来提高模型在推理任务中的性能。CoT 模仿人类的渐进推理过程，通过构建一系列逻辑步骤来解决复杂的视觉推理问题，帮助模型提高他们的深度理解和分析能力。CoT 的有效性已在语言推理任务中得到广泛验证。最近，研究人员自然而然地将其应用扩展到了多模态域。视觉推理任务天生就适合于思维链 （CoT）方法。这些任务要求模型不仅要 “感知” 图像中的内容和上下文，还要 “理解” 这些视觉元素，以做出连贯的推理和决策。因此，多模态 CoT 的探索在研究界得到了显着扩展。大多数现有的多模态 CoT 方法分为两个阶段：决策生成和执行。1）决策生成。这是多模态 CoT 方法的第一步，涉及理解、分析和制定问题的推理计划。现有的确定方法包括将问题分解为子问题 、在图像中捕捉场景图 、查找相关图像的异同等  。他们试图在文本层面简化问题，或在视觉层面添加更多上下文信息。2）执行。在此阶段，模型执行由上一个确定阶段安排的特定操作。具体而言，该模型将规划转化为实际解决方案。现有的执行方法通常依赖于各种专门的 API 工具或视觉语言模型 （VLMs），前者强调任务执行的特殊性，后者强调任务执行的普遍性。虽然这些多模态 CoT 方法提高了视觉推理任务的性能，但仍存在局限性：首先，现有方法在做出决策时，往往直接将纯文本输入到 LLM 中，而不考虑视觉上下文。从直觉上讲，这增加了 LLM 对问题的发散性思维，但实际上，它可能导致 “确定幻觉”。如图 1（a）所示，如果问题本身与图像没有密切关系，而只是根据文本询问 “这个类测量的最高量是多少？”，则 LLM（GPT-3.5）不清楚 “这个类” 的具体含义。它会回答所提供的信息不足，并开始猜测 “类” 是指物理学中的度量还是编程中的类。这种感知的不确定性可能导致 LLM 做出与问题无关的决策，甚至是不正确的、误导性的后续执行，并导致完全不相关的答案。</p>\n<p>其次，在执行过程中，现有方法通常通过调用外部工具来执行任务，因为 MLLM 仍然无法解决众多的视觉推理任务。但这些工具大多是低级的视觉感知工具 （检测器、识别器、OCR 等），只能提取低级的视觉信息。如图 1（b）所示，在比较溶液中的粒子数时，它们仅提供粒子的位置，而无法推断出它们数量之间的关系等高级信息。他们进一步将这些低级线索输入到 LLM 中进行组织和总结 。当复杂的线索增加时，这无疑增加了 LLM 对长文本推理的负担。同时，使用许多外部工具，也增加了管道的复杂性。为了解决上述局限性，我们提出了一种新颖的多模态 CoT 框架 Cantor。在决策生成中，我们使 MLLM 或 LLM 在合唱中充当唱者，同时处理视觉和文本上下文以进行全面理解，然后将特定任务分配给由单个 MLLM 执行的 “专家”，以进行高级逻辑问题解决。具体来说，在决策生成过程中，我们详细分析了视觉信息在决策阶段的重要性。这包括有或没有视觉信息的确定质量，以及详细或简洁的视觉信息对确定的影响的差异。最终，我们得出结论，视觉信息在决策生成阶段至关重要。当我们使用 MLLM 模型 （如 Gemini）作为决策生成器时，我们直接将图像输入到模型中，以充分理解问题并对其进行深思熟虑。然而，当采用 LLM 模型 （如 GPT-3.5）时，我们发现提供更详细的图像标题更有利于理解问题。此外，决策生成器需要明确提供解释性决策，包括解决问题的策略、专家调用的原因以及每个专家的具体任务执行。因此，它指导 MLLM 充当量身定制的专家 （例如 ObjectQuant Locator、TextIntel Extractor、VisionIQAnalyst 和 ChartSense Expert），为流程中的子任务提供结论性答案。如图 1（a）所示，当使用 LLM 做出决定时，在详细的标题指导下，模型知道它要求烧杯的最大体积并做出正确的决定。当图像可供 MLLM 使用时，决策会更加清晰，也就是说，需要 VisionIQAnalyst 提取杯壁顶部的数字。在执行过程中，我们观察到 MLLM 是一种高级认知工具，在直接获取高级信息（例如，相对位置和数量）方面比获取低级视觉信息（如检测位置）性能更好。这种高级信息对于多模态 CoT 来说是优越的。Cantor 没有使用多个外部工具，而是通过不同的专家身份和任务说明将不同的任务分配给单个 MLLM，探索充当某些专家的 MLLM 的专业潜力。量身定做的专家直接提供高水平的专业信息，从而减轻了后续综合推理的负担。如图 1（b）所示，在比较绿色颗粒的浓度时，我们需要先比较两个瓶子中的颗粒数量。MLLM 充当 ObjectQuant 定位器，直接比较两个解决方案中的数量方差。与获取粒子位置相比，MLLM 更准确地获得数量关系的结果。该结果直接应用于最终答案的进一步推断。</p>\n<p>我们提出的框架 Cantor 在 ScinceQA 和 Math\u0002vista 中均实现了 SOTA 结果。当 Gemini 用作决策生成器时，Cantor 分别获得 4.11% 和 5.9% 的精度增益。在 Cantor 中使用 GPT-3.5 还可以实现 2.24% 和 9.2% 的准确度增益。在我们所有的实验中，我们只使用一个 MLLM （Gemini）来扮演多个专家的角色，执行不同的子任务，有不同的要求。我们的贡献如下：</p>\n<ul>\n<li>我们提出了一个鼓舞人心的多模态 CoT 框架，名为 Cantor，它具有感知决策架构，有效地集成了视觉上下文和逻辑推理来解决视觉推理任务。</li>\n<li>我们利用 MLLM 的高级认知能力，充当多方面的专家，获取更高层次的信息并显着增强 CoT 的生成。</li>\n<li>我们在两个具有挑战性的基准测试中证明了 Cantor 的有效性，大大超过了现有的同行。</li>\n</ul>\n",
            "tags": [
                "大模型",
                "CoT",
                "多模态"
            ]
        },
        {
            "id": "https://liuliai.github.io/2024/03/27/MemoryBank%EF%BC%9AEnhancing%20Large%20Language%20Models%20with%20Long-Term%20Memory/",
            "url": "https://liuliai.github.io/2024/03/27/MemoryBank%EF%BC%9AEnhancing%20Large%20Language%20Models%20with%20Long-Term%20Memory/",
            "title": "MemoryBank：Enhancing Large Language Models with Long-Term Memory",
            "date_published": "2024-03-27T09:18:00.000Z",
            "content_html": "<h2>MemoryBank：Enhancing Large Language Models with Long-Term Memory</h2>\n<p>本文是论文<a href=\"https://arxiv.org/abs/2305.10250\" target=\"_blank\" rel=\"noopener\"><strong><font color=\"LimeGreen\">MemoryBank：Enhancing Large Language Models with Long-Term Memory</font></strong></a>的阅读笔记和个人理解，论文来自 AAAI2024. 探讨了一种私人的长期记忆机制，利用持续互动随着时间推移能够适应用户个性的私人 LLM 系统。</p>\n<p>MemoryBank 建立在一个具有内存检索和更新机制的内存存储器上，能够总结过去的事件和用户的个性。通过不断的记忆更新不断进化，通过合成以前交互的信息，随着时间的推移理解和适应用户的个性，允许 LLM 根据经过的时间和记忆的相对重要性来忘记和强化记忆，从而提供更像人类的记忆机制和丰富的用户体验（需要持续互动，如私人伴侣系统、心理咨询和秘书协助）</p>\n<p><strong>包容性强，对 ChatGPT 这样的封闭源代码模型和像 ChatGLM 这样的开放源代码模型方面是通用的</strong></p>\n<p>MemoryBank 思想在作者源码上体现的很简单，就是他每次出现查询请求时，都会遍历一遍历史对话记录，然后当前查询的内容遗忘保留率<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">s+1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> (有具体的数学模型，可以参考链接：<a href=\"https://www.zhihu.com/question/364132423\" target=\"_blank\" rel=\"noopener\"><strong><font color=\"LimeGreen\">https://www.zhihu.com/question/364132423</font></strong></a>，但是作者为了方便简化了)，作者的数学模型就是<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mfrac><mi>t</mi><mi>s</mi></mfrac></mrow></msup></mrow><annotation encoding=\"application/x-tex\">e^{-\\frac{t}{s}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9393400000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9393400000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8233428571428572em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span></span></span></span>，然后计算出这个遗忘强度值，然后用 random 随机数进行比较，当大于就删除这个，小于就保留，就实现了艾宾浩斯记忆曲线可以遗忘和增强记忆的功能。</p>\n<p><img src=\"/image/MemoryBank-fig1.jpg\" alt=\"MemoryBank概述\"></p>\n<h6>记忆存储器（§2.1）存储过去的对话、总结的事件和用户画像，而记忆更新机制（§2.3）更新记忆存储器。记忆检索（§2.2）召回相关记忆。SiliconFriend（第3节）是一个基于LLM的人工智能伴侣，基于MemoryBank。</h6>\n<blockquote>\n<p>摘要</p>\n</blockquote>\n<p>大型语言模型（LLM）的革命性进步极大地重塑了我们与人工智能（AI）系统的互动，在一系列任务中表现出令人印象深刻的性能。尽管如此，一个显著的障碍仍然存在 —— 这些模型中缺乏长期记忆机制。这种不足在需要持续互动的情况下变得越来越明显，如私人伴侣系统、心理咨询和秘书协助。认识到长期记忆的必要性，我们提出了 MemoryBank，这是一种为 LLM 量身定制的新型记忆机制。记忆库使模型能够唤起相关记忆，通过不断的记忆更新不断进化，通过合成以前交互的信息，随着时间的推移理解和适应用户的个性。为了模仿拟人行为并选择性地保存记忆，记忆库引入了一种记忆更新机制，其灵感来自埃宾浩斯遗忘曲线理论。这种机制允许人工智能根据经过的时间和记忆的相对重要性来忘记和强化记忆，从而提供更像人类的记忆机制和丰富的用户体验。MemoryBank 在容纳像 ChatGPT 这样的封闭源代码模型和像 ChatGLM 这样的开放源代码模型方面是通用的。为了验证 MemoryBank 的有效性，我们通过在长期人工智能伴侣场景中创建一个名为 SiliconFriend 的基于 LLM 的聊天机器人来举例说明其应用。进一步调整心理逻辑对话数据，SiliconFriends 在互动中表现出更高的同理心和辨别力。实验包括对真实世界用户对话框的定性分析和对模拟对话框的定量分析。在后者中，ChatGPT 充当具有不同特征的多个用户，并生成涵盖广泛主题的长期对话上下文。我们的分析结果表明，配备 MemoryBank 的 Sili-conFriend 具有很强的长期陪伴能力，因为它可以提供有力的反应，回忆相关记忆，了解用户个性。这突出了 MemoryBank 的有效性</p>\n<blockquote>\n<p>引言</p>\n</blockquote>\n<p>大型语言模型（LLM）的出现，如 ChatGPT（OpenAI，2022）和 GPT-4（Open AI，2023），导致了从教育、医疗保健到客户服务和娱乐等各个行业的影响力不断增加。这些强大的人工智能系统展示了理解和产生类似人类反应的非凡能力。尽管 LLM 具有非凡的能力，但一个关键的局限性是缺乏长期记忆，这是类人沟通的一个重要方面，在需要持续互动的场景中尤其明显，如个人陪伴、心理咨询和秘书任务。人工智能中的长期记忆对于保持上下文理解、确保有意义的交互以及随着时间的推移理解用户行为至关重要。</p>\n<p>例如，个人人工智能同伴需要回忆过去的对话，以建立融洽的关系。在心理咨询中，人工智能可以通过了解用户的历史和过去的情绪状态来提供更有效的支持。同样，秘书人工智能需要记忆来进行任务管理和偏好识别。LLM 中长期记忆的缺失阻碍了它们的性能和用户体验。因此，开发具有改进记忆能力的人工智能系统以实现更无缝和个性化的交互至关重要。</p>\n<p>因此，我们引入了 MemoryBank，这是一种新颖的机制，旨在为 LLM 提供保持长期记忆和绘制用户画像的能力。MemoryBank 使 LLM 能够回忆历史互动，不断发展他们对上下文的理解，并根据过去的互动适应用户的个性，从而提高他们在长期互动场景中的表现。受 Ebbinghaus 遗忘曲线理论的启发，MemoryBank 进一步融入了一种动态记忆机制，该机制密切反映了人类的认知过程。这一机制使人工智能能够记忆、选择性遗忘，并根据逝去的时间加强记忆，提供更自然、更吸引人的用户体验。具体来说，MemoryBank 建立在一个具有内存检索和更新机制的内存存储器上，能够总结过去的事件和用户的个性。</p>\n<p>MemoryBank 是多功能的，因为它既可以容纳像 ChatGPT 这样的封闭源代码 LLM，也可以容纳像 ChapGLM（Zeng et al.，2022）或 BELLE（Yunjie Ji&amp;Li，2023）这样的开源 LLM。</p>\n<p>为了举例说明 MemoryBank 的实际意义，我们开发了 SiliconFriend，这是一款基于 LLM 的人工智能伴侣聊天机器人，与这种创新的记忆机制相集成。SiliconFriend 旨在保留和参考过去的互动，增强 MemoryBank 在打造更具个性的人工智能伴侣方面的变革影响力。SiliconFriend 的一个显著特点是，它对从各种在线来源收集的 38k 个心理对话进行了调整，这使它能够表现出同理心、细心，并提供有用的指导，使它能够熟练地处理充满情感的对话。此外，SiliconFriend 的突出功能之一是通过总结过去的互动来了解用户的个性，这使其能够根据用户的个人特征定制反应，从而增强用户体验。此外，SiliconFriend 支持双语功能，可满足中英文交流用户的需求。这种多语言支持将其可访问性和可用性扩展到不同的用户组。SiliconFriend 通过两个开源模型 ChatGLM 和 BELLE 以及一个闭源模型 ChatGPT 实现，展示了 MemoryBank 在适应不同 LLM 方面的多功能性。</p>\n<p>为了评估 MemoryBank 的有效性，我们进行了包括定性和定量分析的评估，其中前者涉及真实世界的用户对话，后者采用模拟对话。为了进行定量分析，我们创建了一个由 10 天的对话组成的记忆库，这些对话涵盖了各种各样的主题。这些对话涉及 15 个不同性格的虚拟用户，其中 ChatGPT 扮演用户的角色，并根据他们的性格生成对话上下文。基于这种记忆存储，我们设计了 194 个探究性问题，以评估模型是否能够成功地回忆起相关记忆并提供适当的反应。实验结果展示了 SiliconFriend 在记忆回忆、提供移情陪伴和理解用户画像方面的能力。这些发现证实了 MemoryBank 在长期互动场景中显著提高 LLM 性能的潜力。在本文中，我们将主要贡献总结如下：</p>\n<ul>\n<li>我们介绍了 MemoryBank，这是一种新型的类人长期记忆机制，使 LLM 能够存储、回忆、更新记忆和绘制用户画像。</li>\n<li>我们通过 SiliconFriend 展示了 MemoryBank 的实际适用性，SiliconFriends 是一款基于 LLM 的人工智能伴侣，配备了 MemoryBank，并通过心理对话进行调整。它可以回忆过去的记忆，提供同理心的陪伴，并理解用户的行为。</li>\n<li>我们在三个关键方面展示了 MemoryBank 的可推广性：（1）适应开源和闭源 LLM；（2） 具备中英文双语能力；（3） 具有和不具有记忆遗忘机制的适用性。</li>\n</ul>\n",
            "tags": [
                "长期记忆",
                "RAG",
                "大模型"
            ]
        },
        {
            "id": "https://liuliai.github.io/2024/03/10/Python%E5%A4%9A%E6%A0%B8%E5%B9%B6%E8%A1%8C%E6%93%8D%E4%BD%9C/",
            "url": "https://liuliai.github.io/2024/03/10/Python%E5%A4%9A%E6%A0%B8%E5%B9%B6%E8%A1%8C%E6%93%8D%E4%BD%9C/",
            "title": "Python多核并行操作",
            "date_published": "2024-03-10T03:59:00.000Z",
            "content_html": "<h2>Python多核并行操作</h2>\n<p>如果你执行的只是很小的程序，可能单核便能满足你的要求。但是如果你要运行很大的数据类型，比如生物信息学，比如图像的预处理等，这些如果单核处理起来会非常废时间，64 核的 CPU 很多核都没有利用起来。这个时候就需要使用 CPU 的多核并行技术来提升效率。 使用 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>h</mi><mi>t</mi><mi>o</mi><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">htop</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">h</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">p</span></span></span></span> 命令就可以看到服务器的核运行情况。</p>\n<p>Python 默认的多进程、多线程库 multiprocessing and threading 很好用，如果你用的是 Python 默认的解析器 (CPython)，那么你只能放弃使用 threading 库，因为 GIL 的存在，多线程会让你的 Python 程序跑的比单线程还慢。当然，如果你使用的是 JPython 解析器，或者说你的解析器没有 GIL 对你程序的影响，那么你可以尝试去使用多线程。关于 GIL 的更多信息可在此处查看<br>\n<a href=\"https://www.realvnc.com/en/connect/download/viewer/windows/\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">Python 的 GIL 是什么鬼，多线程性能究竟如何</a>（最近的 Python 3.12 新特性可以为每个子解释器单独创建一个 GIL，这样就可以让 Python 充分利用多核的性能，详细信息可见<a href=\"https://cloud.tencent.com/developer/article/2219746\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">Python-3.12 告别 GIL 锁 &amp; 性能原地飞升！</a>和<a href=\"https://www.cnblogs.com/Chang-LeHung/p/17747159.html\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">Python3.12 新特性 ——GIL 重大突破！</a>）</p>\n<p>这里只讲述 Python 库 multiprocessing。<br>\n讲怎么使用之前，需要先明确一个概念。由于 multiprocessing 是一个创建多进程的库，那么如果你要调用的函数在同一个.py 文件下运行的话是不可以的，你分配给进程的函数需要通过 import 导入。例如：</p>\n<pre><code>import multiprocessing\ndef f(x):\n    return x ** 2\npool &#x3D; multiprocessing.Pool(processes&#x3D;5)\ny &#x3D; range(5)\nprint(pool.map(f, y))</code></pre>\n<p>上述程序是不能跑的，因为要调用的函数和 pool 的创建在同一个进程中。正确做法应该是：</p>\n<pre><code>import multiprocessing\ndef f(x):\n    return x ** 2\nif __name__ &#x3D;&#x3D; &#39;__main__&#39;:\n    pool &#x3D; multiprocessing.Pool(processes&#x3D;5)\n    y &#x3D; range(5)\n    print(pool.map(f, y))</code></pre>\n<p>或者：</p>\n<pre><code>import multiprocessing\nfrom fx import f\npool &#x3D; multiprocessing.Pool(processes&#x3D;5)\ny &#x3D; range(5)\nprint(pool.map(f, y))   # [0, 1, 4, 9, 16]</code></pre>\n<p>这样做的目的是为了进程安全，<a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming\" target=\"_blank\" rel=\"noopener\" style=\"color: LimeGreen;\">更多信息</a><br>\n那么，明确了这个问题之后，开始讲一下 multiprocessing 如何使用。</p>\n<h4 id=\"pool对象\"><a class=\"markdownIt-Anchor\" href=\"#pool对象\">#</a> Pool 对象</h4>\n<p>Python 提供了非常简单的 Pool 对象来实现进程池。</p>\n<pre><code>pool &#x3D; multiprocessing.Pool(processes&#x3D;5)    # 创建进程池，并且容纳上限为5个进程。</code></pre>\n<p>而 pool.map (f, y) 是对可迭代对象 y 的每个对象均使用一次 f 函数，并返回每次执行后的数据列表。 与 pool.map () 相似的还有 pool.imap () 和 pool.imap_unordered ()，不同的是后面两个返回的都是迭代器，而最后一个和名字一样，返回的数据是无序的。</p>\n<h4 id=\"process对象\"><a class=\"markdownIt-Anchor\" href=\"#process对象\">#</a> Process 对象</h4>\n<p>具体例子</p>\n<pre><code>import os\nimport multiprocessing\n\n# Main\nprint(&#39;Main:&#39;, os.getpid())\n\n\n# worker function\ndef worker(sign, lock):\n    lock.acquire()\n    print(sign, os.getpid())\n    lock.release()\n\n\n# Multi-process\nrecord &#x3D; []\nlock &#x3D; multiprocessing.Lock()\n\nif __name__ &#x3D;&#x3D; &#39;__main__&#39;:\n    for i in range(5):\n        process &#x3D; multiprocessing.Process(target&#x3D;worker, args&#x3D;(&#39;process&#39;, lock))    # 创建进程对象，把和参数(&#39;process&#39;, lock)赋给函数worker并且让进程执行worker\n        process.start() # 启动进程\n        record.append(process)\n\n    for process in record:\n        process.join()  # 这里可以填入参数，效果为阻塞多少秒，如果为None那么就是终止进程。</code></pre>\n<h5 id=\"output\"><a class=\"markdownIt-Anchor\" href=\"#output\">#</a> output:</h5>\n<pre><code>Main: 96217\nprocess 96300\nprocess 96301\nprocess 96302\nprocess 96303\nprocess 96304</code></pre>\n<p>这是创建进程对象并且让他自己打印自己的 PID，为了防止输出的数据是乱序的，这里加了一把进程锁 lock =multiprocessing.Lock ()，只有拿到锁的进程才能够打印自己的 PID。</p>\n<h4 id=\"pipe和queue\"><a class=\"markdownIt-Anchor\" href=\"#pipe和queue\">#</a> Pipe 和 Queue</h4>\n<p>和名字一样，分别对应操作系统中的管道和消息队列。看两个例子：</p>\n<pre><code>import multiprocessing as mul\n\n\ndef proc1(pipe):\n    pipe.send(&#39;hello&#39;)\n    print(&#39;proc1 rec:&#39;, pipe.recv())\n\n\ndef proc2(pipe):\n    print(&#39;proc2 rec:&#39;, pipe.recv())\n    pipe.send(&#39;hello, too&#39;)\n\n\n# Build a pipe\npipe &#x3D; mul.Pipe()\nif __name__ &#x3D;&#x3D; &#39;__main__&#39;:\n    # Pass an end of the pipe to process 1\n    p1 &#x3D; mul.Process(target&#x3D;proc1, args&#x3D;(pipe[0],))\n    # Pass the other end of the pipe to process 2\n    p2 &#x3D; mul.Process(target&#x3D;proc2, args&#x3D;(pipe[1],))\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()</code></pre>\n<h5 id=\"output-2\"><a class=\"markdownIt-Anchor\" href=\"#output-2\">#</a> output:</h5>\n<pre><code>proc2 rec: hello\nproc1 rec: hello, too</code></pre>\n<p>Pipe 对象建立的时候，返回一个含有两个元素的表，每个元素代表 Pipe 的一端 (Connection 对象)。我们对 Pipe 的某一端调用 send () 方法来传送对象，在另一端使用 recv () 来接收。</p>\n<pre><code>import os\nimport multiprocessing\nimport time\n\ndef inputQ(queue):\n    info &#x3D; str(os.getpid()) + &#39;(put):&#39; + str(time.time())\n    queue.put(info)\n\ndef outputQ(queue,lock):\n    info &#x3D; queue.get()\n    lock.acquire()\n    print (str(os.getpid()) + &#39; get: &#39; + info)\n    lock.release()\n    \nrecord1 &#x3D; []   # store input processes\nrecord2 &#x3D; []   # store output processes\nlock  &#x3D; multiprocessing.Lock()    # To prevent messy print\nqueue &#x3D; multiprocessing.Queue(3)\n\nif __name__ &#x3D;&#x3D; &#39;__main__&#39;:\n    # input processes\n    for i in range(10):\n        process &#x3D; multiprocessing.Process(target&#x3D;inputQ,args&#x3D;(queue,))\n        process.start()\n        record1.append(process)\n    \n    # output processes\n    for i in range(10):\n        process &#x3D; multiprocessing.Process(target&#x3D;outputQ,args&#x3D;(queue,lock))\n        process.start()\n        record2.append(process)\n    \n    for p in record1:\n        p.join()\n    \n    queue.close()  # No more object will come, close the queue\n    \n    for p in record2:\n        p.join()</code></pre>\n<p>一些进程使用 put () 在 Queue 中放入字符串，这个字符串中包含 PID 和时间。另一些进程从 Queue 中取出，并打印自己的 PID 以及 get () 的字符串。</p>\n",
            "tags": [
                "Python",
                "多核并行"
            ]
        },
        {
            "id": "https://liuliai.github.io/2024/03/01/hello-world/",
            "url": "https://liuliai.github.io/2024/03/01/hello-world/",
            "title": "Hello World",
            "date_published": "2024-03-01T09:18:00.000Z",
            "content_html": "<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"quick-start\"><a class=\"markdownIt-Anchor\" href=\"#quick-start\">#</a> Quick Start</h2>\n<h3 id=\"create-a-new-post\"><a class=\"markdownIt-Anchor\" href=\"#create-a-new-post\">#</a> Create a new post</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo new <span class=\"token string\">\"My New Post\"</span></pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"run-server\"><a class=\"markdownIt-Anchor\" href=\"#run-server\">#</a> Run server</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo server</pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"generate-static-files\"><a class=\"markdownIt-Anchor\" href=\"#generate-static-files\">#</a> Generate static files</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo generate</pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"deploy-to-remote-sites\"><a class=\"markdownIt-Anchor\" href=\"#deploy-to-remote-sites\">#</a> Deploy to remote sites</h3>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"><span>h</span></figcaption><table><tr><td data-num=\"1\"></td><td><pre>$ hexo deploy</pre></td></tr></table></figure><p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n",
            "tags": []
        }
    ]
}