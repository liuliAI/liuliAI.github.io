{
    "version": "https://jsonfeed.org/version/1",
    "title": "liuliAI • All posts by \"长期记忆\" tag",
    "description": "",
    "home_page_url": "https://liuliai.github.io",
    "items": [
        {
            "id": "https://liuliai.github.io/2024/03/27/MemoryBank%EF%BC%9AEnhancing%20Large%20Language%20Models%20with%20Long-Term%20Memory/",
            "url": "https://liuliai.github.io/2024/03/27/MemoryBank%EF%BC%9AEnhancing%20Large%20Language%20Models%20with%20Long-Term%20Memory/",
            "title": "MemoryBank：Enhancing Large Language Models with Long-Term Memory",
            "date_published": "2024-03-27T09:18:00.000Z",
            "content_html": "<h2>MemoryBank：Enhancing Large Language Models with Long-Term Memory</h2>\n<p>本文是论文<a href=\"https://arxiv.org/abs/2305.10250\"><strong><font color=\"LimeGreen\">MemoryBank：Enhancing Large Language Models with Long-Term Memory</font></strong></a>的阅读笔记和个人理解，论文来自 AAAI2024. 探讨了一种私人的长期记忆机制，利用持续互动随着时间推移能够适应用户个性的私人 LLM 系统。</p>\n<p>MemoryBank 建立在一个具有内存检索和更新机制的内存存储器上，能够总结过去的事件和用户的个性。通过不断的记忆更新不断进化，通过合成以前交互的信息，随着时间的推移理解和适应用户的个性，允许 LLM 根据经过的时间和记忆的相对重要性来忘记和强化记忆，从而提供更像人类的记忆机制和丰富的用户体验（需要持续互动，如私人伴侣系统、心理咨询和秘书协助）</p>\n<p><strong>包容性强，对 ChatGPT 这样的封闭源代码模型和像 ChatGLM 这样的开放源代码模型方面是通用的</strong></p>\n<p>MemoryBank 思想在作者源码上体现的很简单，就是他每次出现查询请求时，都会遍历一遍历史对话记录，然后当前查询的内容遗忘保留率<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">s+1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> (有具体的数学模型，可以参考链接：<a href=\"https://www.zhihu.com/question/364132423\"><strong><font color=\"LimeGreen\">https://www.zhihu.com/question/364132423</font></strong></a>，但是作者为了方便简化了)，作者的数学模型就是<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mfrac><mi>t</mi><mi>s</mi></mfrac></mrow></msup></mrow><annotation encoding=\"application/x-tex\">e^{-\\frac{t}{s}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9393400000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9393400000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8233428571428572em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span></span></span></span>，然后计算出这个遗忘强度值，然后用 random 随机数进行比较，当大于就删除这个，小于就保留，就实现了艾宾浩斯记忆曲线可以遗忘和增强记忆的功能。</p>\n<p><img src=\"/image/MemoryBank-fig1.jpg\" alt=\"MemoryBank概述\"></p>\n<h6>记忆存储器（§2.1）存储过去的对话、总结的事件和用户画像，而记忆更新机制（§2.3）更新记忆存储器。记忆检索（§2.2）召回相关记忆。SiliconFriend（第3节）是一个基于LLM的人工智能伴侣，基于MemoryBank。</h6>\n<blockquote>\n<p>摘要</p>\n</blockquote>\n<p>大型语言模型（LLM）的革命性进步极大地重塑了我们与人工智能（AI）系统的互动，在一系列任务中表现出令人印象深刻的性能。尽管如此，一个显著的障碍仍然存在 —— 这些模型中缺乏长期记忆机制。这种不足在需要持续互动的情况下变得越来越明显，如私人伴侣系统、心理咨询和秘书协助。认识到长期记忆的必要性，我们提出了 MemoryBank，这是一种为 LLM 量身定制的新型记忆机制。记忆库使模型能够唤起相关记忆，通过不断的记忆更新不断进化，通过合成以前交互的信息，随着时间的推移理解和适应用户的个性。为了模仿拟人行为并选择性地保存记忆，记忆库引入了一种记忆更新机制，其灵感来自埃宾浩斯遗忘曲线理论。这种机制允许人工智能根据经过的时间和记忆的相对重要性来忘记和强化记忆，从而提供更像人类的记忆机制和丰富的用户体验。MemoryBank 在容纳像 ChatGPT 这样的封闭源代码模型和像 ChatGLM 这样的开放源代码模型方面是通用的。为了验证 MemoryBank 的有效性，我们通过在长期人工智能伴侣场景中创建一个名为 SiliconFriend 的基于 LLM 的聊天机器人来举例说明其应用。进一步调整心理逻辑对话数据，SiliconFriends 在互动中表现出更高的同理心和辨别力。实验包括对真实世界用户对话框的定性分析和对模拟对话框的定量分析。在后者中，ChatGPT 充当具有不同特征的多个用户，并生成涵盖广泛主题的长期对话上下文。我们的分析结果表明，配备 MemoryBank 的 Sili-conFriend 具有很强的长期陪伴能力，因为它可以提供有力的反应，回忆相关记忆，了解用户个性。这突出了 MemoryBank 的有效性</p>\n<blockquote>\n<p>引言</p>\n</blockquote>\n<p>大型语言模型（LLM）的出现，如 ChatGPT（OpenAI，2022）和 GPT-4（Open AI，2023），导致了从教育、医疗保健到客户服务和娱乐等各个行业的影响力不断增加。这些强大的人工智能系统展示了理解和产生类似人类反应的非凡能力。尽管 LLM 具有非凡的能力，但一个关键的局限性是缺乏长期记忆，这是类人沟通的一个重要方面，在需要持续互动的场景中尤其明显，如个人陪伴、心理咨询和秘书任务。人工智能中的长期记忆对于保持上下文理解、确保有意义的交互以及随着时间的推移理解用户行为至关重要。</p>\n<p>例如，个人人工智能同伴需要回忆过去的对话，以建立融洽的关系。在心理咨询中，人工智能可以通过了解用户的历史和过去的情绪状态来提供更有效的支持。同样，秘书人工智能需要记忆来进行任务管理和偏好识别。LLM 中长期记忆的缺失阻碍了它们的性能和用户体验。因此，开发具有改进记忆能力的人工智能系统以实现更无缝和个性化的交互至关重要。</p>\n<p>因此，我们引入了 MemoryBank，这是一种新颖的机制，旨在为 LLM 提供保持长期记忆和绘制用户画像的能力。MemoryBank 使 LLM 能够回忆历史互动，不断发展他们对上下文的理解，并根据过去的互动适应用户的个性，从而提高他们在长期互动场景中的表现。受 Ebbinghaus 遗忘曲线理论的启发，MemoryBank 进一步融入了一种动态记忆机制，该机制密切反映了人类的认知过程。这一机制使人工智能能够记忆、选择性遗忘，并根据逝去的时间加强记忆，提供更自然、更吸引人的用户体验。具体来说，MemoryBank 建立在一个具有内存检索和更新机制的内存存储器上，能够总结过去的事件和用户的个性。</p>\n<p>MemoryBank 是多功能的，因为它既可以容纳像 ChatGPT 这样的封闭源代码 LLM，也可以容纳像 ChapGLM（Zeng et al.，2022）或 BELLE（Yunjie Ji&amp;Li，2023）这样的开源 LLM。</p>\n<p>为了举例说明 MemoryBank 的实际意义，我们开发了 SiliconFriend，这是一款基于 LLM 的人工智能伴侣聊天机器人，与这种创新的记忆机制相集成。SiliconFriend 旨在保留和参考过去的互动，增强 MemoryBank 在打造更具个性的人工智能伴侣方面的变革影响力。SiliconFriend 的一个显著特点是，它对从各种在线来源收集的 38k 个心理对话进行了调整，这使它能够表现出同理心、细心，并提供有用的指导，使它能够熟练地处理充满情感的对话。此外，SiliconFriend 的突出功能之一是通过总结过去的互动来了解用户的个性，这使其能够根据用户的个人特征定制反应，从而增强用户体验。此外，SiliconFriend 支持双语功能，可满足中英文交流用户的需求。这种多语言支持将其可访问性和可用性扩展到不同的用户组。SiliconFriend 通过两个开源模型 ChatGLM 和 BELLE 以及一个闭源模型 ChatGPT 实现，展示了 MemoryBank 在适应不同 LLM 方面的多功能性。</p>\n<p>为了评估 MemoryBank 的有效性，我们进行了包括定性和定量分析的评估，其中前者涉及真实世界的用户对话，后者采用模拟对话。为了进行定量分析，我们创建了一个由 10 天的对话组成的记忆库，这些对话涵盖了各种各样的主题。这些对话涉及 15 个不同性格的虚拟用户，其中 ChatGPT 扮演用户的角色，并根据他们的性格生成对话上下文。基于这种记忆存储，我们设计了 194 个探究性问题，以评估模型是否能够成功地回忆起相关记忆并提供适当的反应。实验结果展示了 SiliconFriend 在记忆回忆、提供移情陪伴和理解用户画像方面的能力。这些发现证实了 MemoryBank 在长期互动场景中显著提高 LLM 性能的潜力。在本文中，我们将主要贡献总结如下：</p>\n<ul>\n<li>我们介绍了 MemoryBank，这是一种新型的类人长期记忆机制，使 LLM 能够存储、回忆、更新记忆和绘制用户画像。</li>\n<li>我们通过 SiliconFriend 展示了 MemoryBank 的实际适用性，SiliconFriends 是一款基于 LLM 的人工智能伴侣，配备了 MemoryBank，并通过心理对话进行调整。它可以回忆过去的记忆，提供同理心的陪伴，并理解用户的行为。</li>\n<li>我们在三个关键方面展示了 MemoryBank 的可推广性：（1）适应开源和闭源 LLM；（2） 具备中英文双语能力；（3） 具有和不具有记忆遗忘机制的适用性。</li>\n</ul>\n",
            "tags": [
                "大模型",
                "长期记忆",
                "RAG"
            ]
        }
    ]
}